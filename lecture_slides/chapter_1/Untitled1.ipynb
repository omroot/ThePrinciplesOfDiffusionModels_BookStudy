{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed6dd04-f449-4856-8cc4-400648a097c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c394631-ab51-464c-8c94-7a18f5175420",
   "metadata": {},
   "source": [
    "## Forward KL and Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "A standard choice for the discrepancy between the data distribution and a parametric model $p_\\phi$ is the (forward) Kullback–Leibler divergence:\n",
    "\\begin{align}\n",
    "\\mathcal D_{\\mathrm{KL}}\\!\\left(p_{\\text{data}} \\,\\Vert\\, p_\\phi\\right)\n",
    ":=& \\int p_{\\text{data}}(\\mathbf x)\\,\n",
    "\\log\\!\\frac{p_{\\text{data}}(\\mathbf x)}{p_\\phi(\\mathbf x)}\\, d\\mathbf x \\\\\n",
    "=& \\mathbb E_{\\mathbf x\\sim p_{\\text{data}}}\\!\\Big[\\log p_{\\text{data}}(\\mathbf x) - \\log p_\\phi(\\mathbf x)\\Big].\n",
    "\\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "It is **asymmetric**:\n",
    "$$\n",
    "\\mathcal D_{\\mathrm{KL}}\\!\\left(p_{\\text{data}} \\,\\Vert\\, p_\\phi\\right)\n",
    "\\ne\n",
    "\\mathcal D_{\\mathrm{KL}}\\!\\left(p_\\phi \\,\\Vert\\, p_{\\text{data}}\\right).\n",
    "\\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97b2b1f-d42c-4bf1-bd8d-b0f2e95d7d5b",
   "metadata": {},
   "source": [
    "### Mode covering (intuition)\n",
    "If there exists a set $A$ with positive $p_{\\text{data}}$-mass where $p_\\phi(\\mathbf x)=0$ for $\\mathbf x\\in A$, then the integrand in (1) contains $\\log\\!\\big(p_{\\text{data}}(\\mathbf x)/0\\big)=+\\infty$ on $A$. Hence\n",
    "$$\n",
    "\\mathcal D_{\\mathrm{KL}}\\!\\left(p_{\\text{data}} \\,\\Vert\\, p_\\phi\\right)=+\\infty,\n",
    "$$\n",
    "so minimizing forward KL **forces** the model to assign nonzero probability wherever the data has support—i.e., it encourages *mode covering*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6f444-6688-4f7e-a165-32743c977133",
   "metadata": {},
   "source": [
    "\n",
    "## Decomposing the forward KL\n",
    "\n",
    "Start from (1):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal D_{\\mathrm{KL}}\\!\\left(p_{\\text{data}} \\,\\Vert\\, p_\\phi\\right)\n",
    "&= \\mathbb E_{p_{\\text{data}}}\\!\\left[\\log p_{\\text{data}}(\\mathbf x)\\right]\n",
    "   - \\mathbb E_{p_{\\text{data}}}\\!\\left[\\log p_\\phi(\\mathbf x)\\right] \\\\\n",
    "&= -\\,\\mathbb E_{p_{\\text{data}}}\\!\\left[\\log p_\\phi(\\mathbf x)\\right]\n",
    "   \\;-\\; \\mathcal H\\!\\left(p_{\\text{data}}\\right),\n",
    "\\end{aligned}\n",
    "\\tag{3}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathcal H\\!\\left(p_{\\text{data}}\\right)\n",
    ":= -\\,\\mathbb E_{p_{\\text{data}}}\\!\\left[\\log p_{\\text{data}}(\\mathbf x)\\right]\n",
    "$$\n",
    "is the (Shannon) entropy of the data distribution and **does not depend on $\\phi$**.\n",
    "\n",
    "Equation (3) also shows\n",
    "$$\n",
    "\\mathcal D_{\\mathrm{KL}}\\!\\left(p_{\\text{data}} \\,\\Vert\\, p_\\phi\\right)\n",
    "= \\underbrace{-\\,\\mathbb E_{p_{\\text{data}}}\\!\\left[\\log p_\\phi(\\mathbf x)\\right]}_{\\text{cross-entropy } \\mathcal H(p_{\\text{data}},p_\\phi)}\n",
    "- \\underbrace{\\left(-\\,\\mathbb E_{p_{\\text{data}}}\\!\\left[\\log p_{\\text{data}}(\\mathbf x)\\right]\\right)}_{\\text{entropy } \\mathcal H(p_{\\text{data}})}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa56f1e-ff92-4795-b30a-6ad43c542464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67045819-f764-4f98-bf59-cc247f053857",
   "metadata": {},
   "source": [
    "## Lemma 1.1.1 — Minimizing KL \\(\\Leftrightarrow\\) MLE\n",
    "\n",
    "Because $\\mathcal H(p_{\\text{data}})$ is constant in $\\phi$, minimizing the forward KL is equivalent to maximizing the expected log-likelihood under the data:\n",
    "$$\n",
    "\\boxed{\n",
    "\\min_{\\phi}\\; \\mathcal D_{\\mathrm{KL}}\\!\\left(p_{\\text{data}} \\,\\Vert\\, p_\\phi\\right)\n",
    "\\;\\;\\Longleftrightarrow\\;\\;\n",
    "\\max_{\\phi}\\; \\mathbb E_{\\mathbf x\\sim p_{\\text{data}}}\\!\\left[\\log p_\\phi(\\mathbf x)\\right].\n",
    "}\n",
    "\\tag{4}\n",
    "$$\n",
    "This is precisely **maximum likelihood estimation (MLE)** at the population level.\n",
    "$$\n",
    "\\phi^{*} \\in \\arg\\min_{\\phi}\\, \\mathcal D_{\\mathrm{KL}}\\!\\left(p_{\\text{data}}, p_\\phi\\right)\n",
    "\\quad\\Longleftrightarrow\\quad\n",
    "\\phi^{*} \\in \\arg\\max_{\\phi}\\, \\mathbb E_{p_{\\text{data}}}\\!\\big[\\log p_\\phi(\\mathbf x)\\big].\n",
    "\\tag{5}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c489c77d-1196-4837-94e7-5d39c58e8465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85723ae2-9df8-4afd-8482-52a01e3781c7",
   "metadata": {},
   "source": [
    "## Empirical MLE via Monte Carlo\n",
    "\n",
    "In practice, we cannot evaluate the population expectation in (4) but we do have i.i.d. samples\n",
    "$$\n",
    "\\{\\mathbf x^{(i)}\\}_{i=1}^N \\stackrel{\\text{i.i.d.}}{\\sim} p_{\\text{data}}.\n",
    "$$\n",
    "The Monte Carlo (sample) approximation of the expected log-likelihood is\n",
    "$$\n",
    "\\mathbb E_{p_{\\text{data}}}\\!\\big[\\log p_\\phi(\\mathbf x)\\big]\n",
    "\\;\\approx\\;\n",
    "\\frac{1}{N}\\sum_{i=1}^N \\log p_\\phi\\!\\big(\\mathbf x^{(i)}\\big),\n",
    "\\tag{6}\n",
    "$$\n",
    "leading to the empirical **negative** log-likelihood (NLL) objective:\n",
    "$$\n",
    "\\widehat{\\mathcal L}_{\\mathrm{MLE}}(\\phi)\n",
    ":= -\\,\\frac{1}{N}\\sum_{i=1}^N \\log p_\\phi\\!\\big(\\mathbf x^{(i)}\\big).\n",
    "\\tag{7}\n",
    "$$\n",
    "We then solve\n",
    "$$\n",
    "\\phi_{\\text{MLE}} \\in \\arg\\min_{\\phi}\\, \\widehat{\\mathcal L}_{\\mathrm{MLE}}(\\phi)\n",
    "\\quad\\left(\\equiv \\arg\\max_{\\phi}\\, \\frac{1}{N}\\sum_{i=1}^N \\log p_\\phi(\\mathbf x^{(i)})\\right).\n",
    "\\tag{8}\n",
    "$$\n",
    "\n",
    "**Key point:** optimization of (7) requires only evaluating $p_\\phi$ (and its gradients); **no evaluation of $p_{\\text{data}}(\\mathbf x)$** is needed—only samples from it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7786423-c3d2-414d-b6d2-1f155b914d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06d71ca1-34a7-46a7-8804-bec49172f4a0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal D_{\\mathrm{KL}}\\!\\left(p_{\\text{data}} \\,\\Vert\\, p_\\phi\\right)\n",
    "&= \\int p_{\\text{data}}(\\mathbf x)\\log\\frac{p_{\\text{data}}(\\mathbf x)}{p_\\phi(\\mathbf x)}\\,d\\mathbf x \\\\\n",
    "&= \\int p_{\\text{data}}(\\mathbf x)\\log p_{\\text{data}}(\\mathbf x)\\,d\\mathbf x\n",
    "  - \\int p_{\\text{data}}(\\mathbf x)\\log p_\\phi(\\mathbf x)\\,d\\mathbf x \\\\\n",
    "&= -\\mathcal H(p_{\\text{data}})\n",
    "  + \\mathcal H\\!\\left(p_{\\text{data}},p_\\phi\\right),\n",
    "\\end{aligned}\n",
    "$$\n",
    "where \\(\\mathcal H(p_{\\text{data}},p_\\phi) := -\\mathbb E_{p_{\\text{data}}}[\\log p_\\phi(\\mathbf x)]\\).\n",
    "Since \\(\\mathcal H(p_{\\text{data}})\\) is independent of \\(\\phi\\), minimizing KL over \\(\\phi\\) is the same as minimizing \\(\\mathcal H(p_{\\text{data}},p_\\phi)\\), i.e. maximizing \\(\\mathbb E_{p_{\\text{data}}}[\\log p_\\phi(\\mathbf x)]\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f3a14-c493-4a32-963f-7a991fbb6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Expanded derivations and details\n",
    "\n",
    "### (A) From definition to cross-entropy form\n",
    "\n",
    "\n",
    "### (B) Likelihood of an i.i.d. dataset\n",
    "\n",
    "For i.i.d. data, the joint likelihood factorizes:\n",
    "\\[\n",
    "p_\\phi\\!\\big(\\mathbf x^{(1)},\\dots,\\mathbf x^{(N)}\\big)\n",
    "= \\prod_{i=1}^N p_\\phi\\!\\big(\\mathbf x^{(i)}\\big).\n",
    "\\]\n",
    "Taking logs gives the **log-likelihood**:\n",
    "\\[\n",
    "\\log p_\\phi\\!\\big(\\mathbf x^{(1:N)}\\big)\n",
    "= \\sum_{i=1}^N \\log p_\\phi\\!\\big(\\mathbf x^{(i)}\\big).\n",
    "\\]\n",
    "Dividing by \\(N\\) and negating yields (7). Therefore minimizing NLL equals maximizing the (average) log-likelihood.\n",
    "\n",
    "### (C) Gradient used in practice\n",
    "\n",
    "Assuming we can differentiate through \\(p_\\phi\\),\n",
    "\\[\n",
    "\\nabla_\\phi \\widehat{\\mathcal L}_{\\mathrm{MLE}}(\\phi)\n",
    "= -\\,\\frac{1}{N}\\sum_{i=1}^N \\nabla_\\phi \\log p_\\phi\\!\\big(\\mathbf x^{(i)}\\big).\n",
    "\\]\n",
    "Stochastic gradients use a minibatch \\(\\mathcal B\\subset\\{1,\\dots,N\\}\\):\n",
    "\\[\n",
    "g(\\phi;\\mathcal B)\n",
    "= -\\,\\frac{1}{|\\mathcal B|}\\sum_{i\\in\\mathcal B} \\nabla_\\phi \\log p_\\phi\\!\\big(\\mathbf x^{(i)}\\big),\n",
    "\\]\n",
    "which is an unbiased estimator of the full gradient, enabling SGD/Adam.\n",
    "\n",
    "### (D) Why forward KL is “mode covering”\n",
    "\n",
    "Let \\(S_{\\text{data}}=\\{\\mathbf x: p_{\\text{data}}(\\mathbf x)>0\\}\\) and\n",
    "\\(S_\\phi=\\{\\mathbf x: p_\\phi(\\mathbf x)>0\\}\\).\n",
    "If \\(S_{\\text{data}}\\not\\subseteq S_\\phi\\), then there is a set \\(A\\subseteq S_{\\text{data}}\\setminus S_\\phi\\)\n",
    "with positive \\(p_{\\text{data}}\\)-mass. On \\(A\\), the integrand in (1) equals \\(+\\infty\\),\n",
    "so \\(\\mathcal D_{\\mathrm{KL}}(p_{\\text{data}}\\Vert p_\\phi)=+\\infty\\).\n",
    "Therefore any sequence \\(\\{\\phi_t\\}\\) minimizing the forward KL must eventually satisfy \\(S_{\\text{data}}\\subseteq S_{\\phi_t}\\), pushing \\(p_\\phi\\) to **cover** all data modes.  \n",
    "(Conversely, minimizing the *reverse* KL \\(\\mathcal D_{\\mathrm{KL}}(p_\\phi\\Vert p_{\\text{data}})\\) tends to be *mode seeking*, because it heavily penalizes placing mass where the data has none but does **not** blow up when \\(p_\\phi\\) ignores a data mode.)\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Forward KL:\n",
    "  \\(\\mathcal D_{\\mathrm{KL}}(p_{\\text{data}}\\Vert p_\\phi)\n",
    "  = -\\mathbb E_{p_{\\text{data}}}\\!\\left[\\log p_\\phi(\\mathbf x)\\right] + \\mathcal H(p_{\\text{data}})\\).\n",
    "- Minimizing forward KL over \\(\\phi\\) is **equivalent** to **MLE**:\n",
    "  \\(\\arg\\min_\\phi \\mathcal D_{\\mathrm{KL}}(p_{\\text{data}}\\Vert p_\\phi)\n",
    "  \\Leftrightarrow \\arg\\max_\\phi \\mathbb E_{p_{\\text{data}}}[\\log p_\\phi(\\mathbf x)]\\).\n",
    "- The empirical objective is the average **negative log-likelihood**:\n",
    "  \\(\\widehat{\\mathcal L}_{\\mathrm{MLE}}(\\phi)=-(1/N)\\sum_i \\log p_\\phi(\\mathbf x^{(i)})\\),\n",
    "  optimized with SGD on minibatches—no evaluation of \\(p_{\\text{data}}(\\mathbf x)\\) required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcce8f2-1971-4eca-be8f-a8d2a0cf2905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d127b83-056e-432a-b5dc-7351a1c70f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edba5aaa-e806-4aa1-a5e1-b8f42505aaf9",
   "metadata": {},
   "source": [
    "# Fisher Divergence (Score-Based Modeling)\n",
    "\n",
    "**Definition**\n",
    "$$\n",
    "\\mathcal D_F(p\\Vert q)\n",
    ":= \\mathbb E_{\\mathbf x\\sim p}\\!\\left[ \\left\\| \\nabla_{\\mathbf x}\\log p(\\mathbf x) - \\nabla_{\\mathbf x}\\log q(\\mathbf x) \\right\\|_2^2 \\right].\n",
    "$$\n",
    "\n",
    "**What it measures**\n",
    "- Compares the **score functions** $s_p(\\mathbf x)=\\nabla_{\\mathbf x}\\log p(\\mathbf x)$ and $s_q(\\mathbf x)=\\nabla_{\\mathbf x}\\log q(\\mathbf x)$.\n",
    "- These are vector fields pointing toward regions of **higher probability**.\n",
    "- $\\mathcal D_F(p\\Vert q)\\ge 0$ and equals $0$ iff $p=q$ a.e. (the score fields match).\n",
    "\n",
    "**Why it’s useful**\n",
    "- **Invariant to normalization constants** (depends only on gradients of log-densities), so it works with **unnormalized models**.\n",
    "- Forms the basis of **score matching**: learn a model score $s_\\phi(\\mathbf x)=\\nabla_{\\mathbf x}\\log p_\\phi(\\mathbf x)$ that minimizes\n",
    "$$\n",
    "\\mathbb E_{p_{\\text{data}}}\\!\\left[\\|s_\\phi(\\mathbf x)-s_{p_{\\text{data}}}(\\mathbf x)\\|_2^2\\right].\n",
    "$$\n",
    "- Core to **score-based/diffusion generative models**: take $p=p_{\\text{data}}$ (target) and $q=p_\\phi$ (model), train $s_\\phi$ to align with the data score field, then **sample** using Langevin dynamics / SDEs.\n",
    "\n",
    "**Takeaway**\n",
    "> Fisher divergence trains models to match the **direction & magnitude** of “move-to-higher-density” vectors. Matching these scores is enough to match the **entire distribution**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1157383-4282-48ce-9a76-8a2fcb16269d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab45694-49b7-4b48-8198-fdd380a2d413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80416372-0217-4baf-b251-3d7aaaa7f4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25d79c65-ec84-4dc6-ae62-e57206ad2a23",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "**General $f$-divergence**  \n",
    "$$\n",
    "D_f(p\\Vert q)=\\int q(x)\\,f\\!\\left(\\frac{p(x)}{q(x)}\\right)\\,dx,\\qquad\n",
    "f:\\mathbb R_{+}\\!\\to\\mathbb R\\ \\text{convex},\\ f(1)=0.\n",
    "$$\n",
    "\n",
    "**Examples (choose $f(u)$):**\n",
    "- $f(u)=u\\log u \\ \\Rightarrow\\ D_f=D_{\\mathrm{KL}}(p\\Vert q)$ *(forward KL)*  \n",
    "- $f(u)=\\tfrac12\\!\\left[u\\log u-(u+1)\\log\\!\\tfrac{1+u}{2}\\right] \\ \\Rightarrow\\ D_f=D_{\\mathrm{JS}}(p\\Vert q)$ *(Jensen–Shannon)*\n",
    "- $f(u)=\\tfrac12|u-1|\\ \\Rightarrow\\ D_f=D_{\\mathrm{TV}}(p,q)$ *(total variation)*\n",
    "\n",
    "**Explicit forms**\n",
    "- $D_{\\mathrm{JS}}(p\\Vert q)=\\tfrac12 D_{\\mathrm{KL}}(p\\Vert m)+\\tfrac12 D_{\\mathrm{KL}}(q\\Vert m)$, with $m=\\tfrac12(p+q)$  \n",
    "- $D_{\\mathrm{TV}}(p,q)=\\tfrac12\\int_{\\mathbb R^D}\\!|p(x)-q(x)|\\,dx=\\displaystyle\\sup_{A\\subset\\mathbb R^D}|p(A)-q(A)|$\n",
    "\n",
    "**Intuition / when useful**\n",
    "- **JS:** smooth, symmetric, bounded; balances both distributions (key in GAN theory).  \n",
    "- **TV:** largest possible probability gap across events; sensitive to pointwise differences.  \n",
    "- **KL (forward):** penalizes missing data support $\\Rightarrow$ *mode covering*.  \n",
    "\n",
    "**Optimal transport viewpoint — Wasserstein distance**  \n",
    "Measures the **minimal cost to move mass** from $p$ to $q$ (depends on sample-space geometry, not density ratios). Unlike $f$-divergences, it remains meaningful even when supports of $p$ and $q$ do not overlap.\n",
    "\n",
    "> **Bottom line:** different divergences encode different notions of “closeness,” leading to distinct optimization dynamics and learning behavior in generative modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4421ab41-b623-45e6-b002-3765411beeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104efee2-a310-4218-bc38-fef901efe4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acb7cec6-610d-41c0-974a-85eeafd58e85",
   "metadata": {},
   "source": [
    "# Energy-Based Models (EBMs)\n",
    "\n",
    "**Idea.** EBMs (Ackley et al., 1985; LeCun et al., 2006) define a probability distribution via an **energy** function $E_\\phi(\\mathbf{x})$ that assigns **lower energy to more probable** data.\n",
    "\n",
    "**Density**\n",
    "$$\n",
    "p_\\phi(\\mathbf{x}) \\;=\\; \\frac{1}{Z(\\phi)}\\,\\exp\\!\\big(-E_\\phi(\\mathbf{x})\\big),\n",
    "\\qquad\n",
    "Z(\\phi) \\;=\\; \\int \\exp\\!\\big(-E_\\phi(\\mathbf{x})\\big)\\,d\\mathbf{x}\n",
    "$$\n",
    "\n",
    "- $Z(\\phi)$ is the **partition function** (normalizing constant).\n",
    "\n",
    "**Training**\n",
    "- Typically maximize **log-likelihood** $\\sum_i \\log p_\\phi(\\mathbf{x}^{(i)})$.\n",
    "- Challenge: $Z(\\phi)$ is often **intractable** to compute or differentiate.\n",
    "\n",
    "**Connection to diffusion / score-based models**\n",
    "- Diffusion models learn the **score** $\\nabla_{\\mathbf{x}}\\log p(\\mathbf{x})$ (gradient of log-density), which **does not depend on $Z(\\phi)$**.\n",
    "- This **circumvents** explicit partition-function computation during training and sampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a3567-3fbf-4c62-a9c7-21336806a6fb",
   "metadata": {},
   "source": [
    "# Autoregressive (AR) Models — One-Slide\n",
    "\n",
    "**Idea.** Factorize the joint data distribution using the **chain rule**:\n",
    "$$\n",
    "p_{\\text{data}}(\\mathbf{x})=\\prod_{i=1}^{D} p_\\phi(x_i \\mid \\mathbf{x}_{<i}),\n",
    "\\qquad\n",
    "\\mathbf{x}=(x_1,\\ldots,x_D),\\ \\mathbf{x}_{<i}=(x_1,\\ldots,x_{i-1}).\n",
    "$$\n",
    "\n",
    "**Parameterization.** Each conditional $p_\\phi(x_i \\mid \\mathbf{x}_{<i})$ is a neural net (e.g., **Transformer**), allowing rich dependencies.  \n",
    "- Terms are **normalized by design** (e.g., softmax for discrete, parameterized Gaussian for continuous), so **global normalization is trivial**.\n",
    "\n",
    "**Training.** Maximize exact likelihood (equivalently **minimize NLL**)\n",
    "$$\n",
    "\\max_\\phi \\sum_{n} \\sum_{i=1}^{D} \\log p_\\phi\\!\\big(x_i^{(n)} \\mid \\mathbf{x}_{<i}^{(n)}\\big).\n",
    "$$\n",
    "\n",
    "**Pros**\n",
    "- Strong **density estimation** with **exact likelihoods**.\n",
    "- Flexible conditionals capture complex structure.\n",
    "\n",
    "**Cons**\n",
    "- **Sequential** sampling $\\Rightarrow$ slower generation.\n",
    "- **Fixed ordering** may restrict flexibility.\n",
    "\n",
    "**Takeaway.** Despite sampling limits, AR models are a **foundational** class of likelihood-based generative models and remain central in modern research.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89e5fd5-1031-40f7-9ccf-908aa8b59ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e6ec93-271e-471d-8ea9-a4ef57b1f5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1734f482-d86b-41ba-95fb-7f5e18f269a5",
   "metadata": {},
   "source": [
    "# Variational Autoencoders (VAEs) — One Slide\n",
    "\n",
    "**Idea.** Add latent variables $\\mathbf z$ to capture hidden structure in data $\\mathbf x$.  \n",
    "Learn:\n",
    "- **Encoder** $q_\\theta(\\mathbf z\\mid \\mathbf x)$ (approx. posterior),\n",
    "- **Decoder** $p_\\phi(\\mathbf x\\mid \\mathbf z)$ (generative likelihood),\n",
    "with prior $p_{\\text{prior}}(\\mathbf z)$ (usually $\\mathcal N(0,I)$).\n",
    "\n",
    "**Model.** $p_\\phi(\\mathbf x,\\mathbf z)=p_{\\text{prior}}(\\mathbf z)\\,p_\\phi(\\mathbf x\\mid \\mathbf z)$ and  \n",
    "$\\log p_\\phi(\\mathbf x)\\ \\ge\\ \\mathcal L_{\\text{ELBO}}(\\theta,\\phi;\\mathbf x)$\n",
    "\n",
    "**Training objective (ELBO).**\n",
    "$$\n",
    "\\mathcal L_{\\text{ELBO}}(\\theta,\\phi;\\mathbf x)\n",
    "= \\mathbb E_{\\mathbf z\\sim q_\\theta(\\mathbf z\\mid \\mathbf x)}\n",
    "\\big[\\log p_\\phi(\\mathbf x\\mid \\mathbf z)\\big]\n",
    "\\;-\\;\n",
    "D_{\\text{KL}}\\!\\big(q_\\theta(\\mathbf z\\mid \\mathbf x)\\,\\Vert\\, p_{\\text{prior}}(\\mathbf z)\\big).\n",
    "$$\n",
    "\n",
    "**Term meanings.**\n",
    "- $\\mathbb E[\\log p_\\phi(\\mathbf x\\mid \\mathbf z)]$: **reconstruction** (fit data given latent).  \n",
    "- $D_{\\text{KL}}(q_\\theta\\Vert p_{\\text{prior}})$: **regularization** (keep latents near prior).\n",
    "\n",
    "**Pros.**\n",
    "- Principled likelihood-based learning with **tractable** objective.  \n",
    "- Scales with neural nets; amortized inference via encoder.\n",
    "\n",
    "**Cons.**\n",
    "- Samples can be **less sharp**; training pathologies (e.g., posterior collapse where encoder ignores $\\mathbf z$).\n",
    "\n",
    "**Takeaway.** VAEs fuse neural nets with latent-variable models, set up modern likelihood-based generative modeling, and paved the way for diffusion/score-based methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231628a1-d713-4bd9-ba4a-565356300127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c838799-aed0-44c6-b6e5-de3d8e768a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44f67c90-c560-43b4-80ad-8fb75528d78b",
   "metadata": {},
   "source": [
    "# Normalizing Flows & Neural ODE Flows — One Slide\n",
    "\n",
    "**Idea.** Learn an **invertible** map $f_\\phi:\\mathbf z\\!\\to\\!\\mathbf x$ that pushes a simple base density $p(\\mathbf z)$ (e.g., $\\mathcal N(0,I)$) to a complex data density $p_\\phi(\\mathbf x)$.  \n",
    "- **NFs:** compose bijective layers with tractable Jacobians.  \n",
    "- **NODEs:** model a continuous-time bijection via an ODE.\n",
    "\n",
    "**Change of variables (likelihood)**\n",
    "Let $\\mathbf z=f_\\phi^{-1}(\\mathbf x)$. Then\n",
    "$$\n",
    "\\log p_\\phi(\\mathbf x)\n",
    "= \\log p(\\mathbf z) + \\log\\!\\left|\\det\\!\\left(\\frac{\\partial f_\\phi^{-1}(\\mathbf x)}{\\partial \\mathbf x}\\right)\\right|.\n",
    "$$\n",
    "*Enables exact MLE training.*  \n",
    "(For NODEs, the log-det becomes a time integral of the Jacobian trace along the ODE path.)\n",
    "\n",
    "**Training.** Maximize $\\sum_n \\log p_\\phi(\\mathbf x^{(n)})$; backprop through the invertible layers (or ODE solver).\n",
    "\n",
    "**Pros**\n",
    "- **Exact likelihoods**, no partition function.  \n",
    "- **Invertible sampling/inference:** $\\mathbf z\\!\\leftrightarrow\\!\\mathbf x$.  \n",
    "- Efficient when Jacobian is **triangular/coupling** (e.g., RealNVP, Glow).\n",
    "\n",
    "**Cons**\n",
    "- Architectural constraints for **bijectivity** and **tractable Jacobians** may limit expressivity.  \n",
    "- **NODEs** can be compute-heavy (solver steps, stiffness).  \n",
    "- Scaling to very **high dimensions** can be challenging.\n",
    "\n",
    "**Takeaway.** Flows provide likelihood-based generative models via invertible mappings and the change-of-variables formula; NODEs are their continuous-time counterpart.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37567358-1543-4416-a841-c1ac951b57e4",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks (GANs) — One Slide\n",
    "\n",
    "**Setup.** A **generator** $G_\\phi$ maps noise $z\\sim p_{\\text{prior}}$ to samples $G_\\phi(z)$; a **discriminator** $D_\\xi$ scores real vs. fake.\n",
    "\n",
    "**Min–max objective**\n",
    "$$\n",
    "\\min_{G_\\phi}\\ \\max_{D_\\xi}\\ \n",
    "\\mathbb E_{\\mathbf x\\sim p_{\\text{data}}}\\big[\\log D_\\xi(\\mathbf x)\\big]\n",
    "+\\mathbb E_{z\\sim p_{\\text{prior}}}\\big[\\log\\!\\big(1-D_\\xi(G_\\phi(z))\\big)\\big].\n",
    "$$\n",
    "\n",
    "**Optimal discriminator (for fixed $G_\\phi$)**\n",
    "$$\n",
    "D^{*}(\\mathbf x)=\\frac{p_{\\text{data}}(\\mathbf x)}{p_{\\text{data}}(\\mathbf x)+p_{G_\\phi}(\\mathbf x)}.\n",
    "$$\n",
    "\n",
    "**Generator reduces to Jensen–Shannon (JS) divergence**\n",
    "$$\n",
    "\\min_{G_\\phi}\\ 2\\,D_{\\mathrm{JS}}\\!\\big(p_{\\text{data}}\\Vert p_{G_\\phi}\\big)-\\log 4,\n",
    "\\qquad\n",
    "D_{\\mathrm{JS}}(p\\Vert q)=\\tfrac12 D_{\\mathrm{KL}}(p\\Vert m)+\\tfrac12 D_{\\mathrm{KL}}(q\\Vert m),\n",
    "$$\n",
    "with $m=\\tfrac12(p+q)$.\n",
    "\n",
    "**Interpretation**\n",
    "- GANs **do not** define an explicit density; they **bypass likelihood** and instead *match distributions* via an adversarial game.  \n",
    "- JS link places GANs in the broader **$f$-divergence minimization** family (e.g., $f$-GAN).\n",
    "\n",
    "**Pros / Cons**\n",
    "- Can produce **high-fidelity** samples.  \n",
    "- Training is **unstable** (min–max dynamics; architecture and regularization matter).\n",
    "\n",
    "**Today**\n",
    "- Often used as an **auxiliary** (e.g., adversarial losses) alongside other generative models, including **diffusion**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb7ba1-fac3-4746-b5f6-edf54af37990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_finance",
   "language": "python",
   "name": "dl_finance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
